{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy as laspy\n",
    "import h5py\n",
    "from geojson import Point, Feature, FeatureCollection, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as N\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_label(file):\n",
    "    obj = file.split('_')[0]\n",
    "    if obj == 'truck':\n",
    "        return 0\n",
    "    elif obj == 'jcb':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_normalized_blocks(path, num_points,rgb):\n",
    "    files = os.listdir(path)\n",
    "    print(len(files))\n",
    "    source_file = []\n",
    "    label = []\n",
    "    if rgb:\n",
    "        normalized_blocks = N.zeros((0,num_points,6))\n",
    "        block_min_array = N.zeros((0,6))\n",
    "        block_max_array = N.zeros((0,6))\n",
    "    else:\n",
    "        normalized_blocks = N.zeros((0,num_points,3))\n",
    "        block_min_array = N.zeros((0,3))\n",
    "        block_max_array = N.zeros((0,3))\n",
    "    for file in files:\n",
    "        inFile = laspy.file.File(os.path.join(path,file), mode = 'r')\n",
    "        unscaled_points = N.vstack([inFile.X, inFile.Y, inFile.Z]).transpose()\n",
    "        if(len(list(unscaled_points))<num_points):\n",
    "            print(file)\n",
    "        if(len(list(unscaled_points))>=num_points):\n",
    "            \n",
    "            head = inFile.header\n",
    "            scale = head.scale\n",
    "            offset = head.offset\n",
    "            points = unscaled_points * scale + offset\n",
    "            if rgb:\n",
    "                unscaled_rgb_vals = N.vstack([inFile.Red, inFile.Green, inFile.Blue]).transpose()\n",
    "                rgb_vals = unscaled_rgb_vals/256\n",
    "                points = N.concatenate((points,rgb_vals),axis=1)\n",
    "               \n",
    "            #block_min = head.min\n",
    "            #block_max = head.max\n",
    "            \n",
    "            #sample\n",
    "            random_indexes = N.random.randint(0, high=len(points)-1, size=num_points)\n",
    "            block = points[random_indexes]\n",
    "            \n",
    "            #normalize\n",
    "            block_min = N.min(block ,axis = 0)\n",
    "            block_max = N.max(block ,axis = 0)\n",
    "            \n",
    "            points_sub = block - block_min\n",
    "            diff = N.array(block_max) - N.array(block_min)\n",
    "            n_block = points_sub/diff\n",
    "            normalized_blocks = N.append(normalized_blocks,[n_block],axis=0)\n",
    "            block_min_array = N.append(block_min_array,[block_min],axis=0)\n",
    "            block_max_array = N.append(block_max_array,[block_max],axis=0)\n",
    "            source_file.append(file)            \n",
    "            label.append(find_label(file))  \n",
    "                           \n",
    "    return normalized_blocks,source_file,block_min_array,block_max_array,label   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file has string data!!!!\n",
    "\n",
    "def write_block_to_h5(filename, data, file, block_min_array, block_max_array,label):\n",
    "    hf = h5py.File(filename, 'w')\n",
    "    hf.create_dataset('data', data=data)\n",
    "    asciiList = [n.encode(\"ascii\", \"ignore\") for n in file]\n",
    "    string_type = h5py.special_dtype(vlen=bytes)\n",
    "    hf.create_dataset('source_file', shape = (len(asciiList),1), data = asciiList, dtype=string_type)\n",
    "    hf.create_dataset('min',data = block_min_array)\n",
    "    hf.create_dataset('max',data = block_max_array)\n",
    "    hf.create_dataset('label',data = label)\n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBlockData(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    files = f['source_file'][:]\n",
    "    return (data, files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_block(filename, query_x, query_y):\n",
    "    f = h5py.File(filename)\n",
    "    files = f['source_file'][:]\n",
    "    min_data = f['min'][:]\n",
    "    max_data = f['max'][:]\n",
    "    feature_collection = []\n",
    "    for i in range(min_data.shape[0]):\n",
    "        if(min_data[i][0]<query_x<max_data[i][0] and min_data[i][1]<query_y<max_data[i][1]):\n",
    "            print(files[i][0].decode(\"utf-8\"))\n",
    "            point1 = Point((min_data[i][0],min_data[i][1],min_data[i][2]))\n",
    "            point2 = Point((max_data[i][0],max_data[i][1],max_data[i][2]))\n",
    "                \n",
    "            f1 = Feature(geometry=point1)\n",
    "            feature_collection.append(f1)\n",
    "            f2 = Feature(geometry=point2)\n",
    "            feature_collection.append(f2)\n",
    "\n",
    "            fc = FeatureCollection(feature_collection)\n",
    "            with open('min_max.geojson', 'w') as f:\n",
    "                dump(fc, f)    \n",
    "            return files[i][0].decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(normalized_blocks, source_file, block_min_array, block_max_array, label):\n",
    "    length = len(label)\n",
    "    train_num = 360\n",
    "    eval_num = 48\n",
    "    #eval_num = int(0.15*length)\n",
    "    test_num = length - (train_num + eval_num)\n",
    "    \n",
    "    print(train_num,eval_num,test_num)\n",
    "    \n",
    "    #train data\n",
    "    t_data = normalized_blocks[0:train_num,:]\n",
    "    t_file = source_file[0:(train_num)]\n",
    "    t_label = label[0:(train_num)]\n",
    "    t_block_min_array = block_min_array[0:(train_num),:]\n",
    "    t_block_max_array = block_max_array[0:(train_num),:]\n",
    "    #write_block_to_h5('./training_files/train_rot_20.h5',t_data,t_file,t_block_min_array, t_block_max_array,t_label)\n",
    "    \n",
    "    #evL data\n",
    "    e_data = normalized_blocks[train_num:train_num+eval_num,:]\n",
    "    e_file = source_file[train_num:train_num+eval_num]\n",
    "    e_label = label[train_num:train_num+eval_num]\n",
    "    e_block_min_array = block_min_array[train_num:train_num+eval_num,:]\n",
    "    e_block_max_array = block_max_array[train_num:train_num+eval_num,:]\n",
    "    #write_block_to_h5('./training_files/eval_rot_20.h5',e_data,e_file,e_block_min_array, e_block_max_array,e_label)\n",
    "    \n",
    "    #TEST data\n",
    "    \n",
    "    test_data = normalized_blocks[train_num+eval_num:,:]\n",
    "    test_file = source_file[train_num+eval_num:]\n",
    "    test_label = label[train_num+eval_num:]\n",
    "    test_block_min_array = block_min_array[train_num+eval_num:,:]\n",
    "    test_block_max_array = block_max_array[train_num+eval_num:,:]\n",
    "    #write_block_to_h5('./training_files/test_rot_20.h5',test_data,test_file,test_block_min_array, test_block_max_array,test_label)\n",
    "        \n",
    "    return t_data, t_file, t_label, e_data, e_file, e_label, test_data, test_file, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "360 48 42\n",
      "540\n",
      "360 48 132\n"
     ]
    }
   ],
   "source": [
    "num_points = 1024\n",
    "rgb=False\n",
    "#path = \"/opt/datasets/object_detection/pointnet/files/train_data_3d\"\n",
    "path0 = \"/home/srividya/github/pointnet/training_files/rotated_las_20/trucks\"\n",
    "normalized_blocks, source_file, block_min_array, block_max_array, label = create_normalized_blocks(path0,num_points,rgb)\n",
    "td0,tf0,tl0,ed0,ef0,el0,test_data0, test_file0,test_label0 = split_data(normalized_blocks, source_file, block_min_array, block_max_array, label)\n",
    "\n",
    "path2 = \"/home/srividya/github/pointnet/training_files/rotated_las_20/bg\"\n",
    "normalized_blocks, source_file, block_min_array, block_max_array, label = create_normalized_blocks(path2,num_points,rgb)\n",
    "td2,tf2,tl2,ed2,ef2,el2,test_data2, test_file2,test_label2 = split_data(normalized_blocks, source_file, block_min_array, block_max_array, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = []\n",
    "eval_file = []\n",
    "test_file = []\n",
    "train_label = []\n",
    "test_label = []\n",
    "eval_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tdata = N.concatenate((td0, td2))\n",
    "edata = N.concatenate((ed0, ed2))\n",
    "testdata = N.concatenate((test_data0, test_data2))\n",
    "train_file.extend(tf0)\n",
    "train_file.extend(tf2)\n",
    "eval_file.extend(ef0)\n",
    "eval_file.extend(ef2)\n",
    "test_file.extend(test_file0)\n",
    "test_file.extend(test_file2)\n",
    "train_label.extend(tl0)\n",
    "train_label.extend(tl2)\n",
    "eval_label.extend(el0)\n",
    "eval_label.extend(el2)\n",
    "test_label.extend(test_label0)\n",
    "test_label.extend(test_label2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = [1 if label==2 else label for label in test_label]\n",
    "eval_label = [1 if label==2 else label for label in eval_label]\n",
    "train_label = [1 if label==2 else label for label in train_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_block_to_h5('./training_files/train_rot_20_2class.h5',tdata,train_file,[],[],train_label)\n",
    "write_block_to_h5('./training_files/eval_rot_20_2class.h5',edata,eval_file,[],[],eval_label)\n",
    "write_block_to_h5('./training_files/test_rot_20_2class.h5',testdata,test_file,[],[],test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n"
     ]
    }
   ],
   "source": [
    "print(len(train_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_data(path, step):\n",
    "    files = os.listdir(path)\n",
    "    print(len(files))\n",
    "    for file in files:\n",
    "        inFile = laspy.file.File(os.path.join(path,file), mode = 'r')\n",
    "        unscaled_points = N.vstack([inFile.X, inFile.Y, inFile.Z]).transpose()\n",
    "            \n",
    "        head = inFile.header\n",
    "        scale = head.scale\n",
    "        offset = head.offset\n",
    "        points = unscaled_points * scale + offset\n",
    "        \n",
    "        for z_deg in range(0,360,step):\n",
    "            #print(z_deg)\n",
    "            z = z_deg * N.pi/180\n",
    "            cosz = math.cos(z)\n",
    "            sinz = math.sin(z)\n",
    "\n",
    "            rotation_mat = N.array(\n",
    "                    [[cosz, -sinz, 0],\n",
    "                     [sinz, cosz, 0],\n",
    "                     [0, 0, 1]])\n",
    "            \n",
    "            rotated_points = N.dot(points, rotation_mat)\n",
    "                        \n",
    "            scaled_pts = (rotated_points - offset) / scale\n",
    "            \n",
    "            header_h = laspy.header.Header()\n",
    "            op_file_name = file.split('.')[0]+'_'+str(z_deg)+'.las'\n",
    "            outfile = laspy.file.File(os.path.join('./training_files/rotated_las_20',op_file_name), mode=\"w\", header=head)\n",
    "            outfile.X = scaled_pts[:,0]\n",
    "            outfile.Y = scaled_pts[:,1]\n",
    "            outfile.Z = scaled_pts[:,2]           \n",
    "            outfile.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "path = \"/opt/datasets/object_detection/pointnet/files/train_data_3d\"\n",
    "step = 20\n",
    "rotate_data(path, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3252, 1024)\n"
     ]
    }
   ],
   "source": [
    "block_df = P.read_pickle('feature_0.pkl')\n",
    "#feature = block_df['feature']\n",
    "feature_array = N.load('feature_0.npy')\n",
    "print(feature_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truck_3.las\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/research-disk/virtualenvs/tf-cpu1/lib/python3.6/site-packages/ipykernel_launcher.py:2: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "query_x = 5066.69\n",
    "query_y = 7849.21\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "#tree = KDTree(block_df['feature'].as_matrix())  \n",
    "tree = KDTree(feature_array)\n",
    "block_file = find_block('block_rmz.h5',query_x, query_y)\n",
    "index = block_df['file'] == block_file\n",
    "dist, ind = tree.query(feature_array[index].reshape(1,-1), k=10)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1928  748  719 2973 1930 2821 1411 1373 2395 2403]]\n"
     ]
    }
   ],
   "source": [
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['truck_3.las', 'truck_4.las', 'split_5-14-68.las', 'split_5-45-33.las', 'truck_1.las', 'split_5-17-80.las', 'truck_2.las', 'split_5-58-41.las', 'split_5-41-28.las', 'split_5-27-45.las']\n"
     ]
    }
   ],
   "source": [
    "nn_files = list(block_df['file'][ind[0]])\n",
    "print(nn_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_collection = []\n",
    "nn_files = list(block_df['file'][ind[0]])\n",
    "idx=0\n",
    "for file in nn_files:\n",
    "    inFile = laspy.file.File(os.path.join(ds.path(),file), mode = 'r')\n",
    "    unscaled_points = N.vstack([inFile.X, inFile.Y, inFile.Z]).transpose()\n",
    "    head = inFile.header\n",
    "    scale = head.scale\n",
    "    offset = head.offset\n",
    "    points = unscaled_points * scale + offset\n",
    "    center = N.mean(points,axis=0)\n",
    "    idx=idx+1\n",
    "    point = Point(list(center))    \n",
    "    feature = Feature(geometry=point,id=idx)\n",
    "    feature_collection.append(feature)\n",
    "    \n",
    "fc = FeatureCollection(feature_collection)\n",
    "with open('point_trucks.geojson', 'w') as f:\n",
    "    dump(fc, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4983.84012703 7768.05703727  591.26713637]\n"
     ]
    }
   ],
   "source": [
    "print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import pc_util\n",
    "import imageio\n",
    "\n",
    "for i in range(normalized_blocks.shape[0]):\n",
    "    img = pc_util.point_cloud_three_views(normalized_blocks[i,:,:])\n",
    "    filename = os.path.join('./images',source_file[i].split('.')[0]+'.jpg')\n",
    "    imageio.imwrite(filename,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
