{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    div.output_area img {\n",
       "        max-width: 60em;\n",
       "        margin: auto;\n",
       "        display: block;\n",
       "        object-fit: contain;\n",
       "    }\n",
       "    table.dataframe {\n",
       "        margin: auto;\n",
       "    }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./../../setup.py\n",
    "ds = Dataset('object_detection/photoscan_projects/rmz_nexity/photoscan/cloud_chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy as laspy\n",
    "import h5py\n",
    "from geojson import Point, Feature, FeatureCollection, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_normalized_blocks(path, num_points):\n",
    "    files = os.listdir(path)\n",
    "    source_file = []\n",
    "    normalized_blocks = N.zeros((0,num_points,3))\n",
    "    block_min_array = N.zeros((0,3))\n",
    "    block_max_array = N.zeros((0,3))\n",
    "    for file in files:\n",
    "        inFile = laspy.file.File(os.path.join(path,file), mode = 'r')\n",
    "        unscaled_points = N.vstack([inFile.X, inFile.Y, inFile.Z]).transpose()\n",
    "        if(len(list(unscaled_points))>=num_points):\n",
    "            \n",
    "            head = inFile.header\n",
    "            scale = head.scale\n",
    "            offset = head.offset\n",
    "            points = unscaled_points * scale + offset\n",
    "            block_min = head.min\n",
    "            block_max = head.max\n",
    "            \n",
    "            #sample\n",
    "            random_indexes = N.random.randint(0, high=len(points)-1, size=num_points)\n",
    "            block = points[random_indexes]\n",
    "            \n",
    "            #normalize\n",
    "            #block_min = N.min(block ,axis = 0)\n",
    "            #block_max = N.max(block ,axis = 0)\n",
    "            \n",
    "            points_sub = block - block_min\n",
    "            diff = N.array(block_max) - N.array(block_min)\n",
    "            n_block = points_sub/diff\n",
    "            normalized_blocks = N.append(normalized_blocks,[n_block],axis=0)\n",
    "            block_min_array = N.append(block_min_array,[block_min],axis=0)\n",
    "            block_max_array = N.append(block_max_array,[block_max],axis=0)\n",
    "            source_file.append(file)\n",
    "                                    \n",
    "    return normalized_blocks,source_file,block_min_array,block_max_array   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file has string data!!!!\n",
    "\n",
    "def write_block_to_h5(filename, data, file, block_min_array, block_max_array):\n",
    "    hf = h5py.File(filename, 'w')\n",
    "    hf.create_dataset('data', data=data)\n",
    "    asciiList = [n.encode(\"ascii\", \"ignore\") for n in file]\n",
    "    string_type = h5py.special_dtype(vlen=bytes)\n",
    "    hf.create_dataset('source_file', shape = (len(asciiList),1), data = asciiList, dtype=string_type)\n",
    "    hf.create_dataset('min',data = block_min_array)\n",
    "    hf.create_dataset('max',data = block_max_array)\n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBlockData(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    files = f['source_file'][:]\n",
    "    return (data, files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_block(filename, query_x, query_y):\n",
    "    f = h5py.File(filename)\n",
    "    files = f['source_file'][:]\n",
    "    min_data = f['min'][:]\n",
    "    max_data = f['max'][:]\n",
    "    feature_collection = []\n",
    "    for i in range(min_data.shape[0]):\n",
    "        if(min_data[i][0]<query_x<max_data[i][0] and min_data[i][1]<query_y<max_data[i][1]):\n",
    "            print(files[i][0].decode(\"utf-8\"))\n",
    "            point1 = Point((min_data[i][0],min_data[i][1],min_data[i][2]))\n",
    "            point2 = Point((max_data[i][0],max_data[i][1],max_data[i][2]))\n",
    "                \n",
    "            f1 = Feature(geometry=point1)\n",
    "            feature_collection.append(f1)\n",
    "            f2 = Feature(geometry=point2)\n",
    "            feature_collection.append(f2)\n",
    "\n",
    "            fc = FeatureCollection(feature_collection)\n",
    "            with open('min_max.geojson', 'w') as f:\n",
    "                dump(fc, f)    \n",
    "            return files[i][0].decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3252, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "num_points = 1024\n",
    "normalized_blocks, source_file, block_min_array, block_max_array = create_normalized_blocks(ds.path(),num_points)\n",
    "print(normalized_blocks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_block_to_h5('block_rmz.h5',normalized_blocks,source_file,block_min_array, block_max_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3252, 1024)\n"
     ]
    }
   ],
   "source": [
    "block_df = P.read_pickle('feature_0.pkl')\n",
    "#feature = block_df['feature']\n",
    "feature_array = N.load('feature_0.npy')\n",
    "print(feature_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truck_3.las\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/research-disk/virtualenvs/tf-cpu1/lib/python3.6/site-packages/ipykernel_launcher.py:2: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "query_x = 5066.69\n",
    "query_y = 7849.21\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "#tree = KDTree(block_df['feature'].as_matrix())  \n",
    "tree = KDTree(feature_array)\n",
    "block_file = find_block('block_rmz.h5',query_x, query_y)\n",
    "index = block_df['file'] == block_file\n",
    "dist, ind = tree.query(feature_array[index].reshape(1,-1), k=10)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1928  748  719 2973 1930 2821 1411 1373 2395 2403]]\n"
     ]
    }
   ],
   "source": [
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['truck_3.las', 'truck_4.las', 'split_5-14-68.las', 'split_5-45-33.las', 'truck_1.las', 'split_5-17-80.las', 'truck_2.las', 'split_5-58-41.las', 'split_5-41-28.las', 'split_5-27-45.las']\n"
     ]
    }
   ],
   "source": [
    "nn_files = list(block_df['file'][ind[0]])\n",
    "print(nn_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_collection = []\n",
    "nn_files = list(block_df['file'][ind[0]])\n",
    "idx=0\n",
    "for file in nn_files:\n",
    "    inFile = laspy.file.File(os.path.join(ds.path(),file), mode = 'r')\n",
    "    unscaled_points = N.vstack([inFile.X, inFile.Y, inFile.Z]).transpose()\n",
    "    head = inFile.header\n",
    "    scale = head.scale\n",
    "    offset = head.offset\n",
    "    points = unscaled_points * scale + offset\n",
    "    center = N.mean(points,axis=0)\n",
    "    idx=idx+1\n",
    "    point = Point(list(center))    \n",
    "    feature = Feature(geometry=point,id=idx)\n",
    "    feature_collection.append(feature)\n",
    "    \n",
    "fc = FeatureCollection(feature_collection)\n",
    "with open('point_trucks.geojson', 'w') as f:\n",
    "    dump(fc, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4983.84012703 7768.05703727  591.26713637]\n"
     ]
    }
   ],
   "source": [
    "print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import pc_util\n",
    "import imageio\n",
    "\n",
    "for i in range(normalized_blocks.shape[0]):\n",
    "    img = pc_util.point_cloud_three_views(normalized_blocks[i,:,:])\n",
    "    filename = os.path.join('./images',source_file[i].split('.')[0]+'.jpg')\n",
    "    imageio.imwrite(filename,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
